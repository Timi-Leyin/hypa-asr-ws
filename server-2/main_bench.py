import argparse
from pathlib import Path
from typing import Optional
import time
import librosa
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor


MODEL_ID = "hypaai/wspr_small_2025-11-11_12-12-17"
BASE_MODEL_ID = "openai/whisper-small"


def load_model_and_processor(model_id: str, use_sampling: bool = False, use_compile: bool = False, force_mps: bool = False):
	# Check device availability
	mps_available = torch.backends.mps.is_available()
	
	# Allow forcing MPS even with sampling (user's choice)
	if force_mps and use_sampling:
		use_mps = mps_available
		print("âš ï¸  Warning: Forcing MPS with sampling. May crash if PyTorch version doesn't support it.")
	else:
		use_mps = mps_available and not use_sampling  # Safe default: MPS has sampling issues
	
	device = "mps" if use_mps else "cpu"
	dtype = torch.float16 if use_mps else torch.float32

	print(f"\n{'='*60}")
	print(f"ğŸ–¥ï¸  Device Info:")
	print(f"  - MPS (Apple GPU) available: {mps_available}")
	print(f"  - Using device: {device.upper()}")
	print(f"  - Dtype: {dtype}")
	if use_sampling and not use_mps and mps_available:
		print(f"  âš ï¸  Note: Using CPU for sampling (MPS compatibility issue)")
		print(f"      Reason: torch.multinomial() fails on MPS in many PyTorch versions")
		print(f"      Use --force-mps to try anyway (may crash)")
		print(f"      Or run without sampling flags for faster GPU inference")
	print(f"{'='*60}\n")
	
	print(f"Loading model on {device.upper()} with {dtype}...")
	processor = AutoProcessor.from_pretrained(model_id)
	
	# Load with optimizations
	model = AutoModelForSpeechSeq2Seq.from_pretrained(
		model_id,
		torch_dtype=dtype,
		low_cpu_mem_usage=True,
		use_safetensors=True,
	).to(device)
	
	# Verify model is on correct device
	print(f"âœ“ Model loaded on: {next(model.parameters()).device}")
	
	# Apply BetterTransformer optimization for faster inference
	try:
		model = model.to_bettertransformer()
		print("âœ“ BetterTransformer enabled for faster inference")
	except Exception as e:
		print(f"âš ï¸  BetterTransformer not available: {e}")
	
	# Apply torch.compile for M4 optimization (PyTorch 2.0+)
	if use_compile and hasattr(torch, 'compile'):
		try:
			model.encoder = torch.compile(model.encoder, mode="reduce-overhead")
			print("âœ“ torch.compile enabled for encoder")
		except Exception as e:
			print(f"âš ï¸  torch.compile failed: {e}")

	return model, processor, device


def transcribe_with_loaded_model(
	audio_path: Path,
	model,
	processor,
	device,
	language: Optional[str],
	task: str,
	temperature: float = 0.0,
	top_p: Optional[float] = None,
	top_k: Optional[int] = None,
):
	"""Transcribe audio using a pre-loaded model"""
	# Load and process audio
	import librosa
	audio_array, sampling_rate = librosa.load(str(audio_path), sr=16000)
	duration = len(audio_array) / sampling_rate
	
	inputs = processor(
		audio_array,
		sampling_rate=16000,
		return_tensors="pt",
	)
	
	# Move inputs to device and match model dtype
	input_features = inputs.input_features.to(device).to(model.dtype)

	# Build generation kwargs
	generate_kwargs = {
		"language": language if language else "en",
		"task": task,
	}
	
	if temperature > 0:
		generate_kwargs["do_sample"] = True
		generate_kwargs["temperature"] = temperature
		if top_p is not None:
			generate_kwargs["top_p"] = top_p
		if top_k is not None:
			generate_kwargs["top_k"] = top_k
	else:
		generate_kwargs["do_sample"] = False
	
	# Generate transcription
	with torch.no_grad():
		predicted_ids = model.generate(
			input_features,
			**generate_kwargs,
		)

	# Decode the result
	transcription = processor.batch_decode(
		predicted_ids,
		skip_special_tokens=True,
	)[0]
	
	return transcription.strip(), duration


def transcribe_audio(
	audio_path: Path,
	model_id: str,
	language: Optional[str],
	task: str,
	temperature: float = 0.0,
	top_p: Optional[float] = None,
	top_k: Optional[int] = None,
	use_compile: bool = False,
):
	if not audio_path.exists() or not audio_path.is_file():
		raise FileNotFoundError(f"Audio file not found: {audio_path}")

	print(f"Loading audio from: {audio_path}")
	print(f"File size: {audio_path.stat().st_size / 1024:.1f} KB")
	
	use_sampling = temperature > 0
	model, processor, device = load_model_and_processor(model_id, use_sampling=use_sampling, use_compile=use_compile)

	# Load and process audio

	audio_array, sampling_rate = librosa.load(str(audio_path), sr=16000)
	duration = len(audio_array) / sampling_rate
	print(f"Audio duration: {duration:.2f} seconds")
	
	inputs = processor(
		audio_array,
		sampling_rate=16000,
		return_tensors="pt",
	)
	
	# Move inputs to device and match model dtype
	input_features = inputs.input_features.to(device).to(model.dtype)
	print(f"Input features shape: {input_features.shape}")

	print(f"Running transcription with task='{task}', language='{language or 'en'}'...")
	print(f"Generation params: temperature={temperature}, top_p={top_p}, top_k={top_k}")
	
	# Build generation kwargs
	generate_kwargs = {
		"language": language if language else "en",
		"task": task,
	}
	
	if temperature > 0:
		generate_kwargs["do_sample"] = True
		generate_kwargs["temperature"] = temperature
		if top_p is not None:
			generate_kwargs["top_p"] = top_p
		if top_k is not None:
			generate_kwargs["top_k"] = top_k
	else:
		generate_kwargs["do_sample"] = False
	
	# Generate transcription
	with torch.no_grad():
		predicted_ids = model.generate(
			input_features,
			**generate_kwargs,
		)

	# Decode the result
	print(f"Generated token IDs shape: {predicted_ids.shape}")
	print(f"Generated token IDs: {predicted_ids[0].tolist()}")
	
	transcription = processor.batch_decode(
		predicted_ids,
		skip_special_tokens=True,
	)[0]
	
	print(f"Raw result: {transcription}")
	return transcription.strip(), duration


def parse_args():
	parser = argparse.ArgumentParser(
		description="Run local transcription with a Hugging Face Whisper model."
	)
	parser.add_argument("--audio", required=True, help="Path to local audio file")
	parser.add_argument(
		"--model",
		default=MODEL_ID,
		help=f"Hugging Face model id. Use '{BASE_MODEL_ID}' for the base model (default: {MODEL_ID})",
	)
	parser.add_argument(
		"--language",
		default=None,
		help="Optional language token (example: en, yo).",
	)
	parser.add_argument(
		"--task",
		choices=["transcribe", "translate"],
		default="transcribe",
		help="Whisper generation task",
	)
	parser.add_argument(
		"--temperature",
		type=float,
		default=0.0,
		help="Sampling temperature (0.0 = greedy, higher = more random). Try 0.2-0.8 for variety.",
	)
	parser.add_argument(
		"--top-p",
		type=float,
		default=None,
		help="Nucleus sampling threshold (0.0-1.0). Try 0.95.",
	)
	parser.add_argument(
		"--top-k",
		type=int,
		default=None,
		help="Top-k sampling (e.g., 50). Limits to top k tokens.",
	)
	parser.add_argument(
		"--compile",
		action="store_true",
		help="Enable torch.compile for faster inference on M4 (PyTorch 2.0+).",
	)
	parser.add_argument(
		"--benchmark",
		action="store_true",
		help="Benchmark mode: Load model once and transcribe 3 times to measure inference speed.",
	)
	return parser.parse_args()


def benchmark_mode(args):
	"""Load model once and transcribe the same audio 3 times"""
	audio_path = Path(args.audio)
	if not audio_path.exists():
		raise FileNotFoundError(f"Audio file not found: {audio_path}")
	
	print("\n" + "=" * 60)
	print("ğŸ“Š BENCHMARK MODE: Load once, transcribe 3 times")
	print("=" * 60 + "\n")
	
	# Time model loading
	print("â³ Loading model...")
	load_start = time.time()
	use_sampling = args.temperature > 0
	model, processor, device = load_model_and_processor(
		args.model, 
		use_sampling=use_sampling, 
		use_compile=args.compile
	)
	load_time = time.time() - load_start
	
	print(f"\nâœ“ Model loaded in {load_time:.2f}s\n")
	print("=" * 60)
	
	# Run 3 transcriptions
	transcription_times = []
	audio_duration = 0
	
	for i in range(1, 4):
		print(f"\nğŸ¤ Transcription #{i}...")
		trans_start = time.time()
		text, audio_duration = transcribe_with_loaded_model(
			audio_path=audio_path,
			model=model,
			processor=processor,
			device=device,
			language=args.language,
			task=args.task,
			temperature=args.temperature,
			top_p=args.top_p,
			top_k=args.top_k,
		)
		trans_time = time.time() - trans_start
		transcription_times.append(trans_time)
		
		print(f"âœ“ Transcription #{i} completed in {trans_time:.2f}s")
		if i == 1:
			print(f"Text: {text[:100]}{'...' if len(text) > 100 else ''}")
	
	# Calculate statistics
	total_time = load_time + sum(transcription_times)
	avg_trans_time = sum(transcription_times) / len(transcription_times)
	avg_rtf = avg_trans_time / audio_duration if audio_duration > 0 else 0
	
	# Print summary
	print("\n" + "=" * 60)
	print("ğŸ“Š BENCHMARK RESULTS")
	print("=" * 60)
	print(f"â±ï¸  Model load time: {load_time:.2f}s")
	print(f"ğŸµ Audio duration: {audio_duration:.2f}s")
	print()
	for i, t in enumerate(transcription_times, 1):
		rtf = t / audio_duration if audio_duration > 0 else 0
		print(f"  Transcription #{i}: {t:.2f}s (RTF: {rtf:.2f}x)")
	print()
	print(f"ğŸ“ˆ Average transcription time: {avg_trans_time:.2f}s")
	print(f"âš¡ Average RTF: {avg_rtf:.2f}x {'(faster than real-time)' if avg_rtf < 1 else '(slower than real-time)'}")
	print(f"â±ï¸  Total time (load + 3x transcribe): {total_time:.2f}s")
	print(f"ğŸ’¡ Overhead per run if reloading: +{load_time:.2f}s")
	print("=" * 60 + "\n")


def main():
	args = parse_args()
	
	# Run benchmark mode if requested
	if args.benchmark:
		benchmark_mode(args)
		return
	
	# Normal single transcription mode
	print("\nğŸ¤ Transcribing audio...\n")
	
	start_time = time.time()
	text, audio_duration = transcribe_audio(
		audio_path=Path(args.audio),
		model_id=args.model,
		language=args.language,
		task=args.task,
		temperature=args.temperature,
		top_p=args.top_p,
		top_k=args.top_k,
		use_compile=args.compile,
	)
	elapsed_time = time.time() - start_time
	rtf = elapsed_time / audio_duration if audio_duration > 0 else 0
	
	print("\n" + "=" * 60)
	print("TRANSCRIPTION:")
	print("=" * 60)
	print(text)
	print("=" * 60)
	print(f"â±ï¸  Processing time: {elapsed_time:.2f}s")
	print(f"ğŸµ Audio duration: {audio_duration:.2f}s")
	print(f"âš¡ Real-time factor (RTF): {rtf:.2f}x {'(faster than real-time)' if rtf < 1 else '(slower than real-time)'}\n")


if __name__ == "__main__":
	main()
